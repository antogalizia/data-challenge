
# Pipeline ETL + API con FastAPI 

## DescripciÃ³n

Este proyecto implementa un pipeline ETL que extrae, transforma y limpia datos provistos por una API pÃºblica de Mercado Libre, almacenÃ¡ndolos en las diferentes etapas dentro de la estructura de directorios `raw`, `processed` y `clean`. De manera subyacente, se utiliza FastAPI para exponer los datos mediante una API que se encuentra desplegada en Render.

## Estructura del Proyecto

```
â”œâ”€â”€ src/                
â”‚   â”œâ”€â”€ etl.py         # CÃ³digo fuente del pipeline: clases Extract, Transform, Load
â”‚   â”œâ”€â”€ init.sql       # Script SQL para crear tablas en BigQuery
â”‚
â”œâ”€â”€ data/              # Datos de entrada y salida (cache)
â”‚   â”œâ”€â”€ raw/           # Datos sin procesar (extraÃ­dos)
â”‚   â”œâ”€â”€ processed/     # Datos parseados
â”‚   â”œâ”€â”€ clean/         # Datos limpios
â”‚
â”œâ”€â”€ app/               # ConstrucciÃ³n de la API con FastAPI
â”‚   â”œâ”€â”€ main.py        # Punto de entrada de la API
â”‚   â”œâ”€â”€ routes.py      # DefiniciÃ³n de endpoints
â”‚   â”œâ”€â”€ services.py    # Funciones que invocan los mÃ©todos de las clases
â”‚
â”œâ”€â”€ render.yaml        # ConfiguraciÃ³n para despliegue
â”œâ”€â”€ requirements.txt   # Dependencias del proyecto
â”œâ”€â”€ .gitignore         # Archivos a ignorar por Git (incluye `data/`)
â”œâ”€â”€ README.md          # DocumentaciÃ³n del proyecto
```


## ğŸ”§ Despliegue en Render  

La API estÃ¡ disponible en:  
ğŸ”— [https://data-challenge.onrender.com](https://data-challenge.onrender.com)  

### Endpoints Disponibles

- **Obtener datos extraÃ­dos:**
  ```sh
  curl -X GET "https://data-challenge.onrender.com/extraction"
  ```
- **Obtener datos transformados:**
  ```sh
  curl -X GET "https://data-challenge.onrender.com/processed"
  ```

    En particular tambiÃ©n se puede acceder a `processed/products`, `processed/shipments` y `processed/sellers` para obtener la informaciÃ³n segmentada.
<br>

- **Obtener datos limpios:**
  ```sh
  curl -X GET "https://data-challenge.onrender.com/cleaned"
  ```

  Ãdem Ã­tem anterior para acceder a la informaciÃ³n segmentada.
<br>

### InstalaciÃ³n y uso local

1. Clonar el repositorio.
   ```sh
   git clone https://github.com/antogalizia/data-challenge.git
   cd data-challenge
   ```
2. Crear y activar un entorno virtual.
   ```sh
   python -m venv venv
   source venv/bin/activate  # En macOS/Linux
   venv\Scripts\activate     # En Windows
   ```
3. Instalar dependencias.
   ```sh
   pip install -r requirements.txt
   ```

4. Ejecutar el servidor FastAPI:

    ```sh
    uvicorn app.main:app --reload
    ```

***
## Flujo ETL

### ExtracciÃ³n (`get_data()` en `etl.py`)

- La funciÃ³n `get_data()` estÃ¡ diseÃ±ada para realizar mÃºltiples solicitudes GET a una API y los datos extraÃ­dos son almacenados en `data/raw/` en formato JSON. 
- Se implementa un mecanismo de paginaciÃ³n basado en el parÃ¡metro offset, que se actualiza en cada iteraciÃ³n con un incremento de 50 registros (params.update({"offset": i * 50})).

##### Manejo de Errores.
- Se captura cualquier excepciÃ³n relacionada con la solicitud HTTP usando requests.exceptions.RequestException, lo que permite manejar fallas en la conexiÃ³n o problemas con el servidor.
- Se utiliza response.raise_for_status() para detectar cÃ³digos de error HTTP y lanzar una excepciÃ³n si la respuesta no es exitosa.
- Se captura json.JSONDecodeError en caso de que la API devuelva una respuesta malformada.
  
### TransformaciÃ³n (`parsed_data()`, `cleaning()` en `etl.py`)

- Se leen los datos crudos de la extracciÃ³n con `parsed_data()` para filtrar y parsearlos al modelo relacional propuesto en el archivo `init.sql` para luego almacenarlos en `data/processed/`. Luego, en `cleaning()` se aplican algunas reglas de limpieza sobre los datos y se almacenan en `data/clean/`.

##### ğŸ“Š Modelo Relacional
##### Entidades.
Las entidades identificadas son Productos, Vendedores y EnvÃ­os. Cada una contiene sus propios atributos, entre ellos, sus claves primarias y forÃ¡neas segÃºn corresponda.
##### Relaciones.
Producto - Vendedor: si bien un mismo producto puede ser vendido por mÃºltiples vendedores, se crean publicaciones independientes para vender el mismo producto. Esto significa que cada publicaciÃ³n tiene su propio *product_id*, aunque el producto fÃ­sico sea el mismo. Por lo tanto, cada producto es vendido por un Ãºnico vendedor. 

Vendedor - Producto: un vendedor puede publicar mÃºltiples productos.

Dado que la relaciÃ³n es 1:N, en la tabla de productos (Products) se incluye como clave forÃ¡nea la clave primaria de la tabla de vendedores (Sellers) denominada *seller_id*.  

EnvÃ­o - Producto: cada producto (publicaciÃ³n) tiene un Ãºnico envÃ­o asociado con su publicaciÃ³n. 

Producto - EnvÃ­o: si cada *product_id* corresponde a una publicaciÃ³n Ãºnica, entonces cada publicaciÃ³n tendrÃ¡ exactamente un Ãºnico envÃ­o asociado.

En la tabla de envÃ­os (Shipments) se incluye como clave forÃ¡nea la clave primaria de la tabla de productos (Products) denominada *product_id*. Cada vez que se agregue un nuevo envÃ­o en la tabla de envÃ­os, el valor de *product_id* debe coincidir con un *product_id* vÃ¡lido que ya exista en la tabla de productos para  garantizar la integridad referencial.

### Carga (`load_data()` en `etl.py`)

- La funciÃ³n `load_data` es utilizada para almacenar archivos en formato JSON resultantes de las diferentes etapas del pipeline. 

