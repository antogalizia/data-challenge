
# Pipeline ETL + API con FastAPI 

## Descripci√≥n

Este proyecto implementa un pipeline ETL que extrae, transforma y limpia datos provistos por una API p√∫blica de Mercado Libre, almacen√°ndolos en las diferentes etapas dentro de la estructura de directorios `raw`, `processed` y `clean`. De manera subyacente, se utiliza FastAPI para exponer los datos mediante una API que se encuentra desplegada en Render.

## Estructura del Proyecto

```
‚îú‚îÄ‚îÄ src/                
‚îÇ   ‚îú‚îÄ‚îÄ etl.py         # C√≥digo fuente del pipeline: clases Extract, Transform, Load
‚îÇ   ‚îú‚îÄ‚îÄ init.sql       # Script SQL para crear tablas en BigQuery
‚îÇ
‚îú‚îÄ‚îÄ data/              # Datos de entrada y salida (cache)
‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Datos sin procesar (extra√≠dos)
‚îÇ   ‚îú‚îÄ‚îÄ processed/     # Datos parseados
‚îÇ   ‚îú‚îÄ‚îÄ clean/         # Datos limpios
‚îÇ
‚îú‚îÄ‚îÄ app/               # Construcci√≥n de la API con FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ main.py        # Punto de entrada de la API
‚îÇ   ‚îú‚îÄ‚îÄ routes.py      # Definici√≥n de endpoints
‚îÇ   ‚îú‚îÄ‚îÄ services.py    # Funciones que invocan los m√©todos de las clases
‚îÇ
‚îú‚îÄ‚îÄ render.yaml        # Configuraci√≥n para despliegue
‚îú‚îÄ‚îÄ requirements.txt   # Dependencias del proyecto
‚îú‚îÄ‚îÄ .gitignore         # Archivos a ignorar por Git (incluye `data/`)
‚îú‚îÄ‚îÄ README.md          # Documentaci√≥n del proyecto
```


## üîß Despliegue en Render  

La API est√° disponible en:  
üîó [https://data-challenge.onrender.com](https://data-challenge.onrender.com)  

#### Pasos para autenticarse y consumir la API

Antes de consultar los endpoints, los usuarios deben autenticarse con Mercado Libre para obtener un *access_token*.

1. Al acceder a https://data-challenge.onrender.com ser√°s redirigido/a al enlace de autenticaci√≥n de Mercado Libre donde se debe iniciar sesi√≥n. Consultar por credenciales de prueba.

2. Autorizar la aplicaci√≥n.

3. Redireccionamiento a la generaci√≥n del *access_token* donde el servidor responder√° con un JSON que incluye el mismo.

4. Ahora puedes llamar a los endpoints de la API incluyendo el token de dos maneras:

   - **Desde `curl`**: Incluye el token en el header `Authorization` con el siguiente comando:
     ```sh
     curl -X GET "https://data-challenge.onrender.com/extraction" -H "Authorization: Bearer MI_ACCESS_TOKEN"
     ```
     
   - **Desde el navegador**: Agrega el token como un par√°metro de consulta en la URL:
     ```sh
     https://data-challenge.onrender.com/extraction?access_token=MI_ACCESS_TOKEN
     ```

### Endpoints Disponibles

- **Obtener datos extra√≠dos:**
     ```sh
     curl -X GET "https://data-challenge.onrender.com/extraction" -H "Authorization: Bearer MI_ACCESS_TOKEN"
     ```

- **Obtener datos transformados:**
     ```sh
     curl -X GET "https://data-challenge.onrender.com/processed" -H "Authorization: Bearer MI_ACCESS_TOKEN"
     ```

    En particular tambi√©n se puede acceder a `processed/products`, `processed/shipments` y `processed/sellers` para obtener la informaci√≥n segmentada.
<br>

- **Obtener datos limpios:**
     ```sh
     curl -X GET "https://data-challenge.onrender.com/cleaned" -H "Authorization: Bearer MI_ACCESS_TOKEN"
     ```

  √çdem √≠tem anterior para acceder a la informaci√≥n segmentada.
<br>


***
## Flujo ETL

### Extracci√≥n (`get_data()` en `etl.py`)

- La funci√≥n `get_data()` est√° dise√±ada para realizar m√∫ltiples solicitudes GET a una API y los datos extra√≠dos son almacenados en `data/raw/` en formato JSON. 
- Se implementa un mecanismo de paginaci√≥n basado en el par√°metro offset, que se actualiza en cada iteraci√≥n con un incremento de 50 registros (params.update({"offset": i * 50})).

##### Manejo de Errores.
- Se captura cualquier excepci√≥n relacionada con la solicitud HTTP usando requests.exceptions.RequestException, lo que permite manejar fallas en la conexi√≥n o problemas con el servidor.
- Se utiliza response.raise_for_status() para detectar c√≥digos de error HTTP y lanzar una excepci√≥n si la respuesta no es exitosa.
- Se captura json.JSONDecodeError en caso de que la API devuelva una respuesta malformada.
  
### Transformaci√≥n (`parsed_data()`, `cleaning()` en `etl.py`)

- Se leen los datos crudos de la extracci√≥n con `parsed_data()` para filtrar y parsearlos al modelo relacional propuesto en el archivo `init.sql` para luego almacenarlos en `data/processed/`. Luego, en `cleaning()` se aplican algunas reglas de limpieza sobre los datos y se almacenan en `data/clean/`.

##### üìä Modelo Relacional
##### Entidades.
Las entidades identificadas son Productos, Vendedores y Env√≠os. Cada una contiene sus propios atributos, entre ellos, sus claves primarias y for√°neas seg√∫n corresponda.
##### Relaciones.
Producto - Vendedor: si bien un mismo producto puede ser vendido por m√∫ltiples vendedores, se crean publicaciones independientes para vender el mismo producto. Esto significa que cada publicaci√≥n tiene su propio *product_id*, aunque el producto f√≠sico sea el mismo. Por lo tanto, cada producto es vendido por un √∫nico vendedor. 

Vendedor - Producto: un vendedor puede publicar m√∫ltiples productos.

Dado que la relaci√≥n es 1:N, en la tabla de productos (Products) se incluye como clave for√°nea la clave primaria de la tabla de vendedores (Sellers) denominada *seller_id*.  

Env√≠o - Producto: cada producto (publicaci√≥n) tiene un √∫nico env√≠o asociado con su publicaci√≥n. 

Producto - Env√≠o: si cada *product_id* corresponde a una publicaci√≥n √∫nica, entonces cada publicaci√≥n tendr√° exactamente un √∫nico env√≠o asociado.

En la tabla de env√≠os (Shipments) se incluye como clave for√°nea la clave primaria de la tabla de productos (Products) denominada *product_id*. Cada vez que se agregue un nuevo env√≠o en la tabla de env√≠os, el valor de *product_id* debe coincidir con un *product_id* v√°lido que ya exista en la tabla de productos para  garantizar la integridad referencial.

### Carga (`load_data()` en `etl.py`)

- La funci√≥n `load_data` es utilizada para almacenar archivos en formato JSON resultantes de las diferentes etapas del pipeline. 

